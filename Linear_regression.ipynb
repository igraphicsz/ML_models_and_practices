{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSC311 Lab 3: Linear Regression\n",
    "\n",
    "In this lab, we will complete what we started during lecture 3, and\n",
    "build a linear regression model to predict the air temperature at the UTM pond\n",
    "given information collected at the UTM forest.\n",
    "By the end of this tutorial, you will be able to:\n",
    "\n",
    "1. Write vectorized code using `numpy` to vectorize computation.\n",
    "2. Implement gradient descent to minimize the square loss function of a linear regression model.\n",
    "3. Explain the effect of the learning rate, and what happens if the learning rate is too small/too large.\n",
    "4. Report model accuracy using a validation and test set.\n",
    "\n",
    "Please work in groups of 1-2 during the tutorial.\n",
    "\n",
    "## Acknowledgements\n",
    "\n",
    "It's always great to work with local, UTM data!\n",
    "\n",
    "Data is from https://www.utm.utoronto.ca/geography/resources/meteorological-station/environmental-datasets\n",
    "\n",
    "## Submission\n",
    "\n",
    "If you are working with a partner, start by creating a group on Markus. If you are working alone,\n",
    "click \"Working Alone\".\n",
    "\n",
    "Submit the ipynb file `lab03.ipynb` on Markus \n",
    "**containing all your solutions to the Graded Task**s.\n",
    "Your notebook file must contain your code **and outputs** where applicable,\n",
    "including printed lines and images.\n",
    "Your TA will not run your code for the purpose of grading.\n",
    "\n",
    "For this lab, you should submit the following:\n",
    "\n",
    "- Part 1. Your explanation of why splitting the data by date is better than using a random split. (1 points)\n",
    "- Part 2. Your implementation of `pred` (1 points)\n",
    "- Part 2. Your implementation of `grad`, along with the output of the finite difference tests (3 points)\n",
    "- Part 3. Your implementation of `solve_via_gradient_descent` (1 point)\n",
    "- Part 2. A reasonable hyperparameter choice and training curve for those hyperparameters (1 point)\n",
    "- Part 3. Your code for sklearn to fit the linear regression model (1 point)\n",
    "- Part 3. Your comparison of the results from sklearn vs our own code (1 point)\n",
    "- Part 4. Your explanation for why we should not use the test set to choose between sklearn and our own model (1 point)\n",
    "\n",
    "## Google Colab Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # For plotting\n",
    "import numpy as np              # Linear algebra library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Data\n",
    "\n",
    "Start by running these two lines of code to download the data on to Google Colab.\n",
    "Recall that Google Colab allows users to run certain shell commands, and these\n",
    "shell commands have a `!` at the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download tutorial data files.\n",
    "!wget https://www.cs.toronto.edu/~lczhang/311/lab03/data.zip\n",
    "\n",
    "# Unzip the zip file.\n",
    "!unzip data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to using `numpy` for its linear algebra functionalities, we will again \n",
    "the `pandas` package to help us read CSV files and manipulate tabular data.\n",
    "The below code reads each of the csv files into a **data frame**, which is a way that\n",
    "`pandas` stores tabular data. \n",
    "As an added bonus, Jupyter notebooks display these data frames in a human-readable way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read each of the csv files as a *pandas data frame*\n",
    "data201609 = pd.read_csv('data/data2016sept.csv')\n",
    "data201610 = pd.read_csv('data/data2016oct.csv')\n",
    "data201709 = pd.read_csv('data/data2017sept.csv')\n",
    "data201710 = pd.read_csv('data/data2017oct.csv')\n",
    "\n",
    "# display one the dataframes in the notebook\n",
    "data201609"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also take look at how each of our individual forest features\n",
    "may help predict the pond air temperature. Scatter plots\n",
    "like this give us a sense of how important each of our features may be.\n",
    "If you are curious about what the features mean, refer to the link given\n",
    "in the Acknowledgements section earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = data201609  # change me\n",
    "for fet in [\"forest_air_temp_c\", \"forest_soil_temp_c\", \"forest_rh\", \"forest_soil_wc\"]:\n",
    "    plt.figure()\n",
    "    X = data_set[fet]\n",
    "    t = data_set[\"pond_air_temp_c\"]\n",
    "    plt.title(\"UTM Environmental Data in 2016 Oct/Nov\")\n",
    "    plt.scatter(X, t)\n",
    "    plt.xlabel(fet)\n",
    "    plt.ylabel(\"pond_air_temp_c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Splitting\n",
    "\n",
    "We will first separate the data into training, validation, and test sets.\n",
    "Rather than choosing a random percentage of data points to leave out in our\n",
    "test set, we will instead place the *most recent* data points in our test set.\n",
    "In particular, we will use the `201710` data as the test set, and\n",
    "`201709` data as our validation set.\n",
    "\n",
    "**Graded Task:** Explain why splitting the data by date is a better strategy\n",
    "than randomly selecting data points in our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include your explanation here for your TA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's our code to split the training/validation/test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.concat([data201609, data201610])\n",
    "valid_data = data201709\n",
    "test_data = data201710\n",
    "\n",
    "print(\"Number of training examples:\", len(train_data))\n",
    "print(\"Number of validation examples:\", len(valid_data))\n",
    "print(\"Number of test examples:\", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our training set in place, our main task is to\n",
    "predict the pond air temperature at UTM using the forest features.\n",
    "We need to begin by building the data matrix `X` and the target vector\n",
    "`t` for the training, validation, and test data.\n",
    "\n",
    "**Task**: Complete the function `get_input_targets`, which takes a pandas dataframe.\n",
    "Most of the code is written for you. `X_fets` is a numpy array of shape $N \\times 4$,\n",
    "where each row contains the four forest measurements.\n",
    "You will need to construct the numpy array `X` of shape $N \\times 5$, where\n",
    "`X` has an extra column of 1's. This accounts for the bias parameter.\n",
    "You may find the functions `np.ones` and `np.concatenate` helpful, but\n",
    "make sure you read the documentations before using these functions.\n",
    "\n",
    "Debugging tip: if you find this task challenging, try to keep track of\n",
    "(and print out) the *shapes* of each of the quantities that you work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_targets(data):\n",
    "    \"\"\"\n",
    "    Produces the data matrix and target vector given a dataframe `data` read\n",
    "    from one of the csv files containing the UTM weather data.\n",
    "\n",
    "    The returned data matrix should have a column of 1's, so that the bias \n",
    "    parameter will be folded into the weight vector.\n",
    "    \"\"\"\n",
    "    # extract the target vector\n",
    "    t = np.array(data[\"pond_air_temp_c\"])\n",
    "\n",
    "    # extract the data matrix:\n",
    "    X_fets = np.array(data[[\"forest_air_temp_c\", \"forest_soil_temp_c\", \"forest_rh\", \"forest_soil_wc\"]])\n",
    "\n",
    "    n = len(data) # number of data points, you may find this information useful\n",
    "\n",
    "    X = None # TODO\n",
    "    return (X, t)\n",
    "\n",
    "X_train, t_train = get_input_targets(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print out the shape of `X_train`, just to make sure that it is what we expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are sure that your code works as intended, generate the validation\n",
    "and test data matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, t_valid = get_input_targets(valid_data)\n",
    "X_test, t_test = get_input_targets(test_data)\n",
    "\n",
    "print(X_valid.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Linear Regression Model\n",
    "\n",
    "We will be training a linear regression model using gradient descent.\n",
    "Recall that a linear regression model is of the form:\n",
    "\n",
    "$$y = f({\\bf x}) = {\\bf w}^T {\\bf x}$$\n",
    "\n",
    "Where $y$ is the prediction,\n",
    "${\\bf x}$ is a vector consisting of our features and ${\\bf w}$ is a vector of the trainable weights. Note that we include the bias $b$ as the last element in the weight vector. This is\n",
    "convenient since it allows us to compute $y$ using a single vector-vector multiplication; We don't need to explicitly add in the bias term after the multiplication.\n",
    "Our goal is to find a good set of weights ${\\bf w}$ so that $y$ will be close to $t$\n",
    "across our training data. We will find these weights via gradient descent.\n",
    "\n",
    "Before we can apply gradient descent, we need to write a few helper functions:\n",
    "\n",
    "- `pred(w, X)`: which will compute predictions for a data set\n",
    "- `mse(w, X, t)`: which will compute the mean square error cost function for a data set\n",
    "- `grad(w, X, t)`: which will compute the gradient of the cost function at ${\\bf w}$\n",
    "\n",
    "Since these helper functions will be called many times, we need them to be\n",
    "efficient. We will make sure that these functions are written via **vectorized**\n",
    "`numpy` code. In other words, we will avoid using loops in our code, so that\n",
    "`numpy` can parallelize matrix operations whenever possible.\n",
    "\n",
    "**Graded Task:** Complete the `pred`, which produces a vector of predictions given \n",
    "a set of weights ${\\bf w}$ and a data matrix `X`.\n",
    "Crucially, **do not use any loops**. Instead, use linear algebra\n",
    "operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(w, X):\n",
    "    \"\"\"\n",
    "    Compute the prediction made by a linear hypothesis with weights `w`\n",
    "    on the data set with input data matrix `X`. Recall that N is the number of \n",
    "    samples and D is the number of features. The +1 accounts for the bias term.\n",
    "\n",
    "    Parameters:\n",
    "        `weight` - a numpy array of shape (D+1)\n",
    "        `X` - data matrix of shape (N, D+1)\n",
    "        `t` - target vector of shape (N)\n",
    "\n",
    "\n",
    "    Returns: prediction vector of shape (N)\n",
    "    \"\"\"\n",
    "    # TODO: complete this function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `mse` is written for you, and \n",
    "computes the *cost* associated with a particular choice\n",
    "of ${\\bf w}$ across a data set. It measures how\n",
    "\"bad\" a set of weights ${\\bf w}$ is across a data set. More specifically,\n",
    "it computes the average discrepancy between our predictions based on the input data ($X$) and our weights ${\\bf w}$, and the true\n",
    "values, or targets ($t$). \n",
    "\n",
    "Notice that **we wrote this code without using any loops**.\n",
    "Instead, we used linear algebra operations like matrix multiplication (which can be done using the `@` symbol,\n",
    "or `np.matmul` or `np.dot`).\n",
    "This type of vectorization technique will be key to completing the remainder\n",
    "of this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(w, X, t):\n",
    "    \"\"\"\n",
    "    Compute the mean squared error of a linear hypthesis with weights `w`\n",
    "    on the data set with input data matrix `X` and targets `t`\n",
    "\n",
    "    Parameters:\n",
    "        `weight` - a numpy array of shape (D+1)\n",
    "        `X` - data matrix of shape (N, D+1)\n",
    "        `t` - target vector of shape (N)\n",
    "\n",
    "    Returns: a scalar MSE value\n",
    "    \"\"\"\n",
    "    # This function is already completed for you as an example.\n",
    "    n =  X.shape[0] # the number of data points\n",
    "    y = pred(w, X)  # the vector of predictions\n",
    "    err = y - t\n",
    "    return np.sum(err ** 2) / (2 * n) # compute the MSE in a vectorized way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Graded Task** Now, complete the following function, which computes the\n",
    "gradient of the cost function (square loss) at the weights `w`.\n",
    "Once again, **do not use any loops**. Instead, use linear algebra\n",
    "operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(w, X, t):\n",
    "    '''\n",
    "    Return gradient of the cost function at `w`. The cost function\n",
    "    is the average square loss (MSE) across the data set X and the\n",
    "    target vector t.\n",
    "\n",
    "    Parameters:\n",
    "        `weight` - a current \"guess\" of what our weights should be,\n",
    "                   a numpy array of shape (D+1)\n",
    "        `X` - matrix of shape (N,D+1) of input features\n",
    "        `t` - target y values of shape (N)\n",
    "\n",
    "    Returns: prediction vector of shape (D+1)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we progress in this course, we will find that debugging\n",
    "machine learning code can get extremely challenging.\n",
    "It helps to **be systematic about testing** (follow a system),\n",
    "and to test every helper function as we write it.\n",
    "The `grad` function can be tricky to write, and so it helps to debug\n",
    "this function carefully.\n",
    "\n",
    "One way to debug this function is to recall the definition of a derivative.\n",
    "For a function $g(w): \\mathbb{R} \\rightarrow \\mathbb{R}$,\n",
    "\n",
    "$$g'(w) = \\lim_{h \\rightarrow 0} \\frac{g(w+h) - g(w)}{h}$$\n",
    "\n",
    "This above rule tells us that if we have a way to evaluate `g` and would like to\n",
    "test our implementation of $g'$, we can choose an $h$ small enough, and check if:\n",
    "\n",
    "$$g'(w) \\approx \\frac{g(w+h) - g(w)}{h}$$\n",
    "\n",
    "In our case, we have that for an $h$ small enough, we should have:\n",
    "\n",
    "$$\\frac{\\partial \\textrm{MSE}}{\\partial w_j} \\approx \\frac{\\textrm{MSE}(w_0, w_1, \\dots, w_{j-1}, w_j + h, w_{j+1}, \\dots, w_D) - \\textrm{MSE}(w_0, w_1, \\dots, w_D)}{h}$$\n",
    "\n",
    "**Graded Task:** We will use this above finite difference rule to check our implementation\n",
    "of the `grad` function. Consider the values ${\\bf w} = {\\bf 1}$, where every weight is set to 1.\n",
    "Check, for each of $w_j$, that `grad(w, X_train, t_train)[j]` is close to `(mse(perturbed_w, X_train, t_train) - mse(w, X_train, t_train)) / h`,\n",
    "where `perturbed_w` is a copy of the vector `w` but with the j-th element perturbed by `h`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 0.0001  # a reasonably small value of \"h\"\n",
    "w = np.ones(5)   # a vector of weights\n",
    "for j in [0, 1, 2, 3, 4]:\n",
    "    perturbed_w = np.copy(w)  # start by making a copy of w\n",
    "\n",
    "    # TODO: perturb the jth element of perturbed_w by h\n",
    "    # Be extremely careful that only the j-th element is perturbed!\n",
    "\n",
    "    estimate = 0 # TODO\n",
    "\n",
    "    print(\"Grading Checking for weight j=\", j)\n",
    "    print(\"grad(w)[j]\", grad(w, X_train, t_train)[j])\n",
    "    print(\"(mse(perturbed_w) - mse(w)) / h\", estimate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task** To test our `grad` function more thoroughly, you may wish to repeat the above experiment with different values of ${\\bf w}$,\n",
    "${\\bf X}$ and ${\\bf t}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you are satisfied that your `grad` functions works as intended, we are ready to implement\n",
    "gradient descent.\n",
    "\n",
    "**Graded Task** Complete the function `solve_via_gradient_descent`, as per the specification below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_via_gradient_descent(alpha=0.0025, niter=1000,\n",
    "                               X_train=X_train, t_train=t_train,\n",
    "                               X_valid=X_valid, t_valid=t_valid,\n",
    "                               w_init=None):\n",
    "    '''\n",
    "    Given `alpha` - the learning rate\n",
    "          `niter` - the number of iterations of gradient descent to run\n",
    "          `X_train` - the data matrix to use for training\n",
    "          `t_train` - the target vector to use for training\n",
    "          `X_valid` - the data matrix to use for validation\n",
    "          `t_valid` - the target vector to use for validation\n",
    "          `w_init` - the initial `w` vector (if `None`, use a vector of all zeros)\n",
    "    Solves for linear regression weights.\n",
    "    Return weights after `niter` iterations.\n",
    "    '''\n",
    "    # initialize all the weights to zeros\n",
    "    w = np.zeros(X_train.shape[1])\n",
    "    \n",
    "    # we will track the MSE value at each iteration to record progress\n",
    "    train_mses = []\n",
    "    valid_mses = []\n",
    "\n",
    "    for it in range(niter):\n",
    "        # TODO: update `w` using the gradient descent update rule\n",
    "\n",
    "        # Record the current training and validation MSE values\n",
    "        # Note that in practice, it is expensive to compute MSE at\n",
    "        # every iteration, and practitioners will typically compute cost\n",
    "        # every few iterations instead (e.g. every ~10, 100 or 1000 iterations,\n",
    "        # depending on the learning task)\n",
    "        train_mses.append(mse(w, X_train, t_train))\n",
    "        valid_mses.append(mse(w, X_valid, t_valid))\n",
    "\n",
    "    plt.title(\"Training Curve Showing Training and Validation MSE at each Iteration\")\n",
    "    plt.plot(train_mses, label=\"Training MSE\")\n",
    "    plt.plot(valid_mses, label=\"Validation MSE\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Final Training MSE:\", train_mses[-1])\n",
    "    print(\"Final Validation MSE:\", valid_mses[-1])\n",
    "\n",
    "    return w\n",
    "\n",
    "solve_via_gradient_descent(alpha=0.0001, niter=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task** Show an example of a learning rate that is too low (makes little progress). Do this by calling `solve_via_gradient_descent()` with your chosen learning rate and number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task** Show an example of a learning rate that is too high (cost does not decrease at all). Do this by calling `solve_via_gradient_descent()` with your chosen learning rate and number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Graded Task** At what learning rate and training iteration do you think we should stop training?\n",
    "Choose a learning rate and `niter` setting that minimizes the **validation MSE**.\n",
    "Here, we are treating the `niter` and `alpha` settings as **hyperparamters**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.0001 # TODO: choose your own value\n",
    "niter = 200    # TODO: choose your own value\n",
    "best_w = solve_via_gradient_descent(alpha=alpha, niter=niter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. Linear Regression via `sklearn`\n",
    "\n",
    "In practise, machine learning practitioners rarely code models from scratch.\n",
    "Instead, we use libraries where commonly used model and code are already written\n",
    "and optimized for us.\n",
    "\n",
    "**Graded Task**: Fit the `LinearRegression` model below using our data set. You will need \n",
    "to read the documentation for the `sklearn.linear_model.LinearRegression` class\n",
    "to do so: [https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression(fit_intercept=False)\n",
    "\n",
    "# TODO: fit the model\n",
    "\n",
    "print(lr.coef_) # weights\n",
    "print(\"Training MSE:\", mse(lr.coef_ , X_train, t_train))\n",
    "print(\"Validation MSE:\", mse(lr.coef_ , X_valid, t_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Graded Task**: Compare your results with these ones. Do you get the same results?\n",
    "Does sklearn's result give a higher or lower training MSE than yours?\n",
    "What about validation MSE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4. Reporting Test Accuracy\n",
    "\n",
    "**Graded Task**: Compare your best weights (in terms of validation MSE) from Parts 2 and 3.\n",
    "How should you choose which model to use? Why is it a **bad idea** to use the test set to\n",
    "make this decision?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose a single best hypothesis from Parts 3 and 4.\n",
    "Reporting the test accuracy on this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
