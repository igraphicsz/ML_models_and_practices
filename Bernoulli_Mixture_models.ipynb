{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a81-gwZ7qKLD"
      },
      "source": [
        "# CSC311 Lab 10: Clustering via Bernoulli Mixture Models\n",
        "\n",
        "In this lab, we will build a Bernoulli Mixture Model to cluster\n",
        "movie reviews. We will use the same movie reviews data set as in Lab 9.\n",
        "However, we will *not* use the positive/negative labels. Instead,\n",
        "we will analyze the reviews themselves to see whether there are\n",
        "interesting structure in the reviews, in the form of clusters.\n",
        "\n",
        "By the end of this lab , you will be able to:\n",
        "\n",
        "1. Explain the connection between the Bernoulli Mixture Model and the Naive Bayes model from week 8.\n",
        "2. Explain the connection between the Bernoulli Mixture Model and the Gaussian Mixture model from lecture 10.\n",
        "3. Apply the E-M algorithm to fit a BMM model to perform clustering over Bernoulli variables.\n",
        "4. Track the progress of the E-M algorithm by tracking the MLE.\n",
        "5. Analyze the results of the clustering by manually observing the clusters.\n",
        "\n",
        "Please work in groups of 1-2 during the lab.\n",
        "\n",
        "## Submission\n",
        "\n",
        "If you are working with a partner, start by creating a group on Markus.\n",
        "If you are working alone,\n",
        "click \"Working Alone\".\n",
        "\n",
        "Submit the ipynb file `lab10.ipynb` on Markus\n",
        "**containing all your solutions to the Graded Task**s.\n",
        "Your notebook file must contain your code **and outputs** where applicable,\n",
        "including printed lines and images.\n",
        "Your TA will not run your code for the purpose of grading.\n",
        "\n",
        "For this lab, you should submit the following:\n",
        "\n",
        "\n",
        "- Part 2. your implementation of the `e_step` function (3 points).\n",
        "- Part 2. your implementation of the `m_step` function (3 points).\n",
        "- Part 2. your implementation of the `em_bmm` function (1 points).\n",
        "- Part 3. your code and interpretation of the results of running `em_bmm` on all data, with 2 clusters (2 points)\n",
        "- Part 3. your code and interpretation of the results of running `em_bmm` on positive reviews, with 5 clusters (1 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ki3It6n6qKLE"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqiIB1H-qKLE"
      },
      "source": [
        "## Acknowledgements\n",
        "\n",
        "Data is a variation of the one from https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews,\n",
        "pre-processed so that only 1000 words are in the training/test set.\n",
        "\n",
        "## Part 1. Data\n",
        "\n",
        "We will use the same movie review data set as in lab 9."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MWOFBItqKLE",
        "outputId": "4d15895a-fecf-410d-8986-e6de8404652b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-28 07:54:31--  https://www.cs.toronto.edu/~lczhang/311/lab09/trainvalid.csv\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1464806 (1.4M) [text/csv]\n",
            "Saving to: ‘trainvalid.csv.1’\n",
            "\n",
            "trainvalid.csv.1    100%[===================>]   1.40M  8.99MB/s    in 0.2s    \n",
            "\n",
            "2025-03-28 07:54:31 (8.99 MB/s) - ‘trainvalid.csv.1’ saved [1464806/1464806]\n",
            "\n",
            "--2025-03-28 07:54:31--  https://www.cs.toronto.edu/~lczhang/311/lab09/test.csv\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 491538 (480K) [text/csv]\n",
            "Saving to: ‘test.csv.1’\n",
            "\n",
            "test.csv.1          100%[===================>] 480.02K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-03-28 07:54:31 (4.19 MB/s) - ‘test.csv.1’ saved [491538/491538]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download tutorial data files.\n",
        "!wget https://www.cs.toronto.edu/~lczhang/311/lab09/trainvalid.csv\n",
        "!wget https://www.cs.toronto.edu/~lczhang/311/lab09/test.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "0xwURRyWqKLE"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "# Training/Validation set\n",
        "trainfile = \"trainvalid.csv\"\n",
        "data = list(csv.reader(open(trainfile)))\n",
        "\n",
        "# Build the vocabulary\n",
        "vocab = set()\n",
        "for review, label in csv.reader(open(trainfile)):\n",
        "    words = review.split()\n",
        "    for w in words:\n",
        "        vocab.add(w)\n",
        "vocab = list(vocab) # len(vocab) == 1000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kE6itbeqKLF"
      },
      "source": [
        "**Task**: Use the function `make_bow` that you wrote in lab 9 to create a\n",
        "data matrix consisting of bag-of-word features. We will then create three\n",
        "matrices: one for all reviews, one for only positive reviews, and one\n",
        "for only negative reviews."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "1Jr0TvCuqKLF"
      },
      "outputs": [],
      "source": [
        "def make_bow(data, vocab):\n",
        "    \"\"\"\n",
        "    Produce the bag-of-word representation of the data, along with a vector\n",
        "    of labels. You *may* use loops to iterate over `data`. However, your code\n",
        "    should not take more than O(len(data) * len(vocab)) to run.\n",
        "\n",
        "    Parameters:\n",
        "        `data`: a list of `(review, label)` pairs, like those produced from\n",
        "                `list(csv.reader(open(\"trainvalid.csv\")))`\n",
        "        `vocab`: a list consisting of all unique words in the vocabulary\n",
        "\n",
        "    Returns:\n",
        "        `X`: A data matrix of bag-of-word features. This data matrix should be\n",
        "             a numpy array with shape [len(data), len(vocab)].\n",
        "             Moreover, `X[i,j] == 1` if the review in `data[i]` contains the\n",
        "             word `vocab[j]`, and `X[i,j] == 0` otherwise.\n",
        "        `t`: A numpy array of shape [len(data)], with `t[i] == 1` if\n",
        "             `data[i]` is a positive review, and `t[i] == 0` otherwise.\n",
        "    \"\"\"\n",
        "    X = np.zeros([len(data), len(vocab)])\n",
        "    t = np.zeros([len(data)])\n",
        "\n",
        "    # TODO: fill in the appropriate values of X and t\n",
        "    vocab_index = {word: idx for idx, word in enumerate(vocab)}\n",
        "\n",
        "    for i, (review, label) in enumerate(data):\n",
        "      words = set(review.split())\n",
        "      for word in words:\n",
        "            if word in vocab_index:\n",
        "                X[i, vocab_index[word]] = 1\n",
        "      t[i] = 1 if label == \"positive\" else 0\n",
        "\n",
        "    return X, t\n",
        "\n",
        "# Data matrix with all reviews in trainvalid.csv\n",
        "X_all, t_all = make_bow(data, vocab)\n",
        "\n",
        "# Data matrix with only positive reviews\n",
        "data_pos = [data[i] for i in np.where(t_all == 1)[0]] # actual review text\n",
        "X_pos = X_all[t_all == 1]                             # data matrix\n",
        "\n",
        "# Data matrix with only negative reviews\n",
        "data_neg = [data[i] for i in np.where(t_all == 0)[0]] # actual review text\n",
        "X_neg = X_all[t_all == 0]                             # data matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ux84zWB0qKLF"
      },
      "source": [
        "## Part 2. Bernoulli Mixture Model\n",
        "\n",
        "A Bernoulli Mixture Model is a finite mixture of random binary, independent features.\n",
        "$$p({\\bf x})=  \\sum_{k=1}^K p({\\bf x} | z=k) \\ P(z=k) = (\\sum_{k=1}^K \\prod_{j=1}^D p(x_j | z=k) ) P(z=k)$$\n",
        "Where $p(z)$ describes the distribution of a categorical random variable,\n",
        "and each $p(x_j | z=k)$ describes the distribution of a Bernoulli random variable.\n",
        "The parameters of our model include $\\pi_k=P(z=k)$ (if there are $K$ classes,\n",
        "then we have parameters $\\pi_1$, $\\pi_2$, \\dots, $\\pi_{K-1}$), and\n",
        "$\\theta_{jk}=p(x_j=1 |z=k)$ (of which there are $DK$ in total).\n",
        "\n",
        "Note the similarities and differences between a BMM v.s.\n",
        "a mixture of Gaussians described in class (written MoG, also Gaussian Mixture Model or GMM).\n",
        "In both mixture models, we have\n",
        "$$p({\\bf x})= p({\\bf x} | z=k) \\ P(z=k)$$,\n",
        "with $P(z)$ being a categorical distribution. The difference is that in\n",
        "a mixture of Gaussians, $p({\\bf x} | z=k)$ is a Gaussian, whereas  in a\n",
        "Bernoulli mixture model, $p({\\bf x} | z=k) = \\prod_{j=1}^D p(x_j | z=k)$.\n",
        "That is, each $p({\\bf x} | z=k)$ consists of $D$ different conditionally\n",
        "independent Bernoullis $p(x_j | z=k)$, where ${\\bf x} \\in \\mathbb{R}^D$.\n",
        "\n",
        "The decomposition of\n",
        "$p({\\bf x})= \\sum_{k=1}^K (\\prod_{j=1}^D p(x_j | z=k) ) P(z=k)$\n",
        "is similar to Naive Bayes. However,\n",
        "unlike in the Naive Bayes lab, our random variable $z$ is **not observed**. Thus,\n",
        "$z$ is called a **latent variable**.\n",
        "\n",
        "<!--\n",
        "Still, we can marginalize out $z$ to write\n",
        "down the likelihood of our data:\n",
        "\\begin{align*}\n",
        "\\ell(\\{\\pi_k, \\theta_{jk}\\})\n",
        "    &= \\sum_{k=1}^K \\sum_{i=1}^N \\log\\Big\\{ p({\\bf x}^{(i)}|z^{(i)})p(z^{(i)}=k)\\Big\\}\\\\\n",
        "    &= \\sum_{k=1}^K \\sum_{i=1}^N \\log \\Big\\{p(z^{(i)}=k) \\prod_{j=1}^D p(x_j^{(i)} | z^{(i)}=k) \\Big\\}\\\\\n",
        "    &= \\sum_{k=1}^K \\sum_{i=1}^N \\left[ \\log p(z^{(i)}=k) + \\sum_{j=1}^D \\log p(x_j^{(i)} | z^{(i)}=k) \\right] \\\\\n",
        "    &= \\sum_{k=1}^K \\sum_{i=1}^N \\log p(z^{(i)}=k) + \\sum_{k=1}^K \\sum_{j=1}^D \\sum_{i=1}^N \\log p(x_j^{(i)} | z^{(i)}=k)\n",
        "\\end{align*}\n",
        "-->\n",
        "\n",
        "In this section, we will fit the parameters of this BMM model to maximize\n",
        "the likelihood of our movie review data.\n",
        "\n",
        "Because $z$ is latent, we will need to use the Expectation-Maximization (E-M) algorithm.\n",
        "EM is an optimization approach where we alternatively optimize the\n",
        "responsibilities $r_k^{(i)}= P(z^{(i)}=k | {\\bf x}^{(i)}; \\theta, \\pi)$ given the parameters,\n",
        "and optimize the parameters given the cluster responsibilities $r_k^{(i)}$ for each data point $i$\n",
        "and cluster $k$.\n",
        "\n",
        "**Graded Task**: The \"E\" step of the Expectation-Maximization Algorithm\n",
        "assign the **responsibility** $r_k^{(i)}$ of component $k$ for data point $i$ using the posterior probability:\n",
        "$$r_k^{(i)}= P(z^{(i)}=k | {\\bf x}^{(i)}; \\theta, \\pi)$$\n",
        "Complete the function `e_step` which performs this computation.\n",
        "You **may** use loops, but minimizing the use of loops would help make your code run faster.\n",
        "\n",
        "If you are not sure where to start, a good place is to review the \"E\" step for\n",
        "the Gaussian mixture model. Computation, the \"E\" step for the GMM is the *inference*\n",
        "stage of a Gaussian Discriminate Analysis model. Likewise, the \"E\" step of our\n",
        "BMM model is computationally the same as the *inference* stage of a Naive Bayes\n",
        "model. If this analogy doesn't makes sense, please ask!\n",
        "Once this analogy makes sense, it should be clear what computation needs to be\n",
        "done, and how this math relates to your work from lab 9."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oe7HBcOFqKLF",
        "outputId": "1ec47137-f0b2-4134-8c73-7f42a452cd7e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.81818182, 0.18181818],\n",
              "       [0.11111111, 0.88888889]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "def e_step(X, pi, theta):\n",
        "    \"\"\"\n",
        "    Perform the \"E\" step of the E-M Algorithm for a BMM.\n",
        "    In other words, given the data matrix `X` and estimates to the\n",
        "    parameters `theta` and `pi`, estimate $P(z=k|{\\bf x})$ for each\n",
        "    ${\\bf x}$ in the data matrix.\n",
        "\n",
        "    Parameters:\n",
        "        `X` - a data matrix of bag-of-word features of shape [N, D],\n",
        "              where N is the number of data points and D is the vocabulary size.\n",
        "              X[i,j] should be either 0 or 1. Produced by the make_bow() function.\n",
        "        `pi` - a vector of shape [K], where `pi[k]` corresponds to\n",
        "              an estimate of the parameter $\\pi_{k} = P(z=k)$.\n",
        "              Precondition: `np.sum(pi) = 1` so that the $\\pi_k$s describe a\n",
        "              probability distribution.\n",
        "        `theta` - a matrix of shape [D, K], where `theta[j, k]` corresponds to\n",
        "              an estimate of the parameter $\\theta_{jk} = P(x_j = 1 | z=k)$\n",
        "\n",
        "    Returns:\n",
        "        `R` - a matrix of shape [N, K], where `R[j, k]` corresponds to the\n",
        "              value $P(z^{(j)}=k| {\\bf x^{(j)}})$ computed using the estimated\n",
        "              parameters `theta` and `pi`.\n",
        "              Each row of `R` should sum up to 1, i.e. `sum(R[j,:]) == 1`.\n",
        "    \"\"\"\n",
        "    N, D = X.shape\n",
        "    D, K = theta.shape\n",
        "    R = np.zeros([N, K])\n",
        "\n",
        "    # TODO: fill in the elements of R\n",
        "    # Remember the log trick that we used to avoid computing a product\n",
        "    # of small numbers, from lab 9.\n",
        "\n",
        "    eps = 1e-10  # to avoid log(0)\n",
        "    log_theta = np.log(theta + eps)           # shape: [D, K]\n",
        "    log_one_minus_theta = np.log(1 - theta + eps)\n",
        "\n",
        "    for j in range(N):\n",
        "        x_j = X[j]  # shape: [D]\n",
        "        log_prob = np.zeros(K)\n",
        "        for k in range(K):\n",
        "            log_likelihood = np.dot(x_j, log_theta[:, k]) + np.dot(1 - x_j, log_one_minus_theta[:, k])\n",
        "            log_prob[k] = np.log(pi[k] + eps) + log_likelihood\n",
        "\n",
        "        # Stable softmax (manual log-sum-exp)\n",
        "        max_log = np.max(log_prob)\n",
        "        exp_shifted = np.exp(log_prob - max_log)\n",
        "        R[j] = exp_shifted / np.sum(exp_shifted)\n",
        "\n",
        "    return R\n",
        "\n",
        "# Test example with N=2 movie reviews, D=1 words, K=2 clusters\n",
        "X_basic_check = np.array([[1], [0]]) # first data point has the word present\n",
        "                                     # second data point has the word absent\n",
        "pi_basic_check = np.array([0.5, 0.5]) # both clusters have equal probability\n",
        "theta_basic_check = np.array([[0.9 ,  # first cluster has P(word present|cluster) = 0.9\n",
        "                               0.2]]) # second cluster has P(word present|cluster) = 0.2\n",
        "\n",
        "# Please include the output of the below call in your submission.\n",
        "# Think: what do the 4 results numbers here mean? You should\n",
        "# see that the first data point has a higher probability of\n",
        "# being in the first cluster (vs the second cluster), and vice-versa\n",
        "# for the second data point. Why would this make sense?\n",
        "# (You do not need to write a response. The question is here to\n",
        "# help you interpret what this function is doing.)\n",
        "e_step(X_basic_check, pi_basic_check, theta_basic_check)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bpj0nZGqKLF"
      },
      "source": [
        "**Graded Task**: The \"M\" step of the Expectation-Maximization Algorithm\n",
        "computes estimates of $\\pi$ and $\\theta_{jk}$ via maximum likelihood,\n",
        "where each the parameters for each class $k$ is fit with a weighted dataset.\n",
        "The weights are proportional to the responsibilities.\n",
        "Complete the function `m_step` which performs this computation.\n",
        "You **may** use loops, but minimizing the use of loops would help make your code run faster.\n",
        "\n",
        "If you are not sure where to start, a good place is to review the \"M\" step for\n",
        "the Gaussian mixture model. Computation, the \"M\" step for the GMM is\n",
        "a slight variation of the *learning* stage of a Gaussian Discriminate Analysis model.\n",
        "Likewise, the \"E\" step of our BMM model is computationally very similar as the\n",
        "*learning* stage of a Naive Bayes model (except that $r_j^{(i)}$ is now continuous\n",
        "rather than being exactly 0 or 1).\n",
        "Once again, if this analogy doesn't makes sense, please ask!\n",
        "And when this analogy makes sense, it should be clear what computation\n",
        "needs to be done, and how this math relates to your work from lab 9."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-tQkXxHqKLF",
        "outputId": "04ec2473-797e-48d5-c504-2a74d295fff9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0.6, 0.4]), array([[0.58333333, 0.375     ]]))"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "def m_step(X, R):\n",
        "    \"\"\"\n",
        "    Perform for a BMM the \"M\" step of the E-M Algorithm for a BMM.\n",
        "    In other words, given the data matrix `X` and estimates to the\n",
        "    parameters `theta` and `pi`, estimate $P(z=k|{\\bf x})$ for each ${\\bf x}$ in the\n",
        "    data matrix.\n",
        "\n",
        "    Parameters:\n",
        "        `X` - a data matrix of bag-of-word features of shape [N, D],\n",
        "              where N is the number of data points and D is the vocabulary size.\n",
        "              X[i,j] should be either 0 or 1. Produced by the make_bow() function.\n",
        "        `R` - a matrix responsibilities of shape [N, K], where `R[j, k]` corresponds\n",
        "              to the value $P(z^{(j)}=k| {\\bf x^{(j)}})$ computed during the e_step.\n",
        "              Precondition: Each row of `R` sums to 1, i.e. `sum(R[j,:]) == 1`\n",
        "\n",
        "    Returns:\n",
        "        `theta` - a matrix of shape [D, K], where `theta[j, k]` corresponds to\n",
        "              the MLE estimate of the parameter $\\theta_{jk} = p(x_j = 1 | z=k)$\n",
        "        `pi` - a vector of shape [K], where `pi[k]` corresponds to\n",
        "              the MLE estimate of the parameter $\\pi_{k} = P(z=k)$.\n",
        "              We should have `np.sum(pi) = 1` so that the $\\pi_k$s describe a\n",
        "              probability distribution.\n",
        "    \"\"\"\n",
        "    N, D = X.shape\n",
        "    N, K = R.shape\n",
        "\n",
        "    pi =  np.sum(R, axis=0) / N\n",
        "    weighted_sum = X.T @ R\n",
        "    cluster_totals = np.sum(R, axis=0)\n",
        "    theta =  weighted_sum / cluster_totals # TODO: fill this!\n",
        "\n",
        "\n",
        "    return (pi, theta)\n",
        "\n",
        "# Test example with N=2 movie reviews, D=1 words, K=2 clusters\n",
        "X_basic_check = np.array([[1], [0]]) # first data point has the word present\n",
        "                                     # second data point has the word absent\n",
        "R_basic_check = np.array([[0.7, 0.3],\n",
        "                          [0.5, 0.5]])\n",
        "\n",
        "# Please include the output of the below call in your submission.\n",
        "# Think: you should see that the \"pi\"s are such that the first\n",
        "# cluster is a tad more likely to be observed. Why does this\n",
        "# make sense? You should also see that the estimate for\n",
        "# P(word present|cluster 1) is higher than P(word present|cluster 2).\n",
        "# Why does this also make sense? (You do not need to write a response.\n",
        "# The question is here to help you interpret what this function is doing.)\n",
        "m_step(X_basic_check, R_basic_check)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwUCe21MqKLF"
      },
      "source": [
        "**Graded Task**: Complete the function `em_bmm` by calling the functions `e_step`\n",
        "and `m_step` that you wrote previously."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "mkv-BW1kqKLF"
      },
      "outputs": [],
      "source": [
        "def em_bmm(X, K=5, num_iter=10):\n",
        "    \"\"\"\n",
        "    Use the E-M algorithm to compute parameters `theta` and `pi` of a\n",
        "    Bernouli Mixture Model fit on the data `X`.\n",
        "\n",
        "    Parameters:\n",
        "        `X` - a data matrix of bag-of-word features of shape [N, D],\n",
        "              where N is the number of data points and D is the vocabulary size.\n",
        "              X[i,j] should be either 0 or 1. Produced by the make_bow() function.\n",
        "        `K` - number of classes (i.e. number of Bernouli Mixtures in the model)\n",
        "        `num_iter` - number of iterations to run the E-M algorithm\n",
        "\n",
        "    Returns:\n",
        "        `theta` - a matrix of shape [D, K], where `theta[j, k]` corresponds to\n",
        "              the E-M estimate of the parameter $\\theta_{jk} = P(x_j = 1 | z=k)$\n",
        "        `pi` - a vector of shape [K], where `pi[k]` corresponds to\n",
        "              the E-M estimate of the parameter $\\pi_{k} = P(z=k)$.\n",
        "              We should have `np.sum(pi) = 1` so that the $\\pi_k$s describe a\n",
        "              probability distribution.\n",
        "    \"\"\"\n",
        "    N, vocab_size = X.shape\n",
        "\n",
        "    pi = np.ones([K]) * (1/K)  # start with equal class probabilities\n",
        "    theta = np.random.rand(vocab_size, K) / 10 # initialize p(x_j|z) to be between 0 and 0.1\n",
        "\n",
        "    for j in range(num_iter):\n",
        "        # e-step\n",
        "        # TODO - what needs to be computed here?\n",
        "        R = e_step(X, pi, theta)\n",
        "        # m-step\n",
        "        # TODO - what needs to be computed here?\n",
        "        pi, theta = m_step(X, R)\n",
        "\n",
        "    return pi, theta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckM847esqKLF"
      },
      "source": [
        "## Part 3. Clustering\n",
        "\n",
        "In this section, we will use the `em_bmm` to cluster our movie reviews. We will\n",
        "*not* use the positive/negative reviews as labels, and the clusters that we find\n",
        "may or may not correspond to easily interpretable results.\n",
        "\n",
        "**Graded Task**: Fit a BMM model using the EM algorithm on the entire data (`X_all`).\n",
        "Use `K=2` and `num_iter=10` to cluster the data into 2 classes.\n",
        "Then, use `e_step` to compute the class responsibilities `Z`, for each data point.\n",
        "After you have computed `Z`, use the below code to print out some example reviews\n",
        "in each of the two classes. Do these classes correspond to positive/negative reviews?\n",
        "If not, how would you describe the difference between the two clusters?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5m7FAzIqKLF",
        "outputId": "c379d43d-9ca2-4de5-e0c9-3901254299b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======= Samples From Class 0 =========\n",
            "this movie has some things that are pretty amazing first it is supposed to be based on a true story the problem was that by then you would be able to hear it\n",
            "lets start with the good things this is not a movie for kids the action scenes are great here too ok over to the things i didnt like thats that\n",
            "i enjoyed this movie as a kid when it came out and to this day still do i am surprised not too many people know about it cool flick\n",
            "i love this film it is well written and acted and has good cinematography see it buy it show it to your friends it still made me happy when everything worked out well in the end\n",
            "ever since i was a child this has been one of my favorite stories i want nothing from you why cant we be friends it is a moving performance and one of the movies most dramatic scenes not\n",
            "you want a friend get a dog at first all good no bad right everything goes to hell of course love that guy on some level i like that take the kids\n",
            "but the plot i heard was so great was so predictable plus the only cool character gets killed off in the middle of the movie wrong in short not much happens\n",
            "the end of an era its a shame that this movie is however not among their best the premise of the movie sounds good and is good most of the jokes in the movie still work good but the movie just however never gets truly hilarious or memorable\n",
            "it is a little known film even to die hard horror fans but i found this movie pretty entertaining dont get me wrong its certainly not without its problems but do think more people should give it a look i thought the acting which seemed forced at first somehow got better as the film went along it seems like the actors really got into their roles\n",
            "after all arent movies meant to be a good time the movie itself was an all around good time just dont expect to have to think too much about it because then if you take it too seriously then the movie actually wont be fun but stupid instead\n",
            "======= Samples From Class 1 =========\n",
            "im not sure i havent seen every episode but i still enjoyed it its hard to say which episode was my favorite\n",
            "this film has everything ask yourself are you a fan of and ive seen a lot of movies\n",
            "its not going to be the japanese version the show is great but\n",
            "i would love to see either of them in another movie\n",
            "this movie is great if you enjoy watching b class movies that is\n",
            "if you can do that it really is good s s s s i doubt it too bad\n",
            "first of all i want to point on screen play\n",
            "this is the best movie i have ever seen any movie that can get children to learn history is great\n",
            "but it is still dont care is it worth try it\n",
            "while by no means a classic the directors involved do have an idea what suspense is\n"
          ]
        }
      ],
      "source": [
        "pi, theta = em_bmm(X_all, K=2, num_iter=10)\n",
        "Z = e_step(X_all, pi, theta)\n",
        "\n",
        "classes = np.argmax(Z, axis=1)\n",
        "for k in range(Z.shape[1]):\n",
        "    print(\"======= Samples From Class\", k, \"=========\")\n",
        "    for i in np.where(classes == k)[0][:10]:\n",
        "        print(data[i][0])\n",
        "\n",
        "# TODO: Write your interpretation of the classes here\n",
        "# Class 0 reviews tend to be more neutral or mixed in tone, with some expressing uncertainty or qualified praise.\n",
        "# Class 1 reviews are generally more enthusiastic, emotional, and expressive, often reflecting strong personal enjoyment.\n",
        "# Thus, the clustering seems to separate casual/mildly positive reviews (Class 0) from strongly positive or emotionally engaged reviews (Class 1)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFzwyfxJqKLF"
      },
      "source": [
        "**Graded Task**: Fit a BMM model using the EM algorithm on only the positive\n",
        "reviews (`X_pos`).\n",
        "Use `K=5` and `num_iter=10` to cluster the data into 5 classes.\n",
        "Then, use `e_step` to compute the class responsibilities `Z`, for each data point.\n",
        "After you have computed `Z`, use the below code to print out some example reviews\n",
        "in each of the five classes.\n",
        "\n",
        "Provide an interpretation for what these classes mean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7UYpyQZqKLF",
        "outputId": "36830d33-d6d5-4fa1-a304-48bcbdbb440d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======= Samples From Class 0 =========\n",
            "this movie has some things that are pretty amazing first it is supposed to be based on a true story the problem was that by then you would be able to hear it\n",
            "lets start with the good things this is not a movie for kids the action scenes are great here too ok over to the things i didnt like thats that\n",
            "i love this film it is well written and acted and has good cinematography see it buy it show it to your friends it still made me happy when everything worked out well in the end\n",
            "ever since i was a child this has been one of my favorite stories i want nothing from you why cant we be friends it is a moving performance and one of the movies most dramatic scenes not\n",
            "you want a friend get a dog at first all good no bad right everything goes to hell of course love that guy on some level i like that take the kids\n",
            "the end of an era its a shame that this movie is however not among their best the premise of the movie sounds good and is good most of the jokes in the movie still work good but the movie just however never gets truly hilarious or memorable\n",
            "it is a little known film even to die hard horror fans but i found this movie pretty entertaining dont get me wrong its certainly not without its problems but do think more people should give it a look i thought the acting which seemed forced at first somehow got better as the film went along it seems like the actors really got into their roles\n",
            "after all arent movies meant to be a good time the movie itself was an all around good time just dont expect to have to think too much about it because then if you take it too seriously then the movie actually wont be fun but stupid instead\n",
            "i just love the movie the movie was excellent and its a very good actor so it deserves that and more there are some actors that do not act very well but have a oscar anyway it does not matter but he is perfect brilliant and beautiful\n",
            "i will be watching this movie many many more times find a friend who does and watch it there but for those times just boys got it today i hope some of it goes to girls\n",
            "======= Samples From Class 1 =========\n",
            "im not sure i havent seen every episode but i still enjoyed it its hard to say which episode was my favorite\n",
            "this film has everything ask yourself are you a fan of and ive seen a lot of movies\n",
            "its not going to be the japanese version the show is great but\n",
            "i would love to see either of them in another movie\n",
            "this movie is great if you enjoy watching b class movies that is\n",
            "first of all i want to point on screen play\n",
            "this is the best movie i have ever seen any movie that can get children to learn history is great\n",
            "the film is unique in many ways this film will be two hours of your time well spent\n",
            "one of my most favorite films this is a real moment in time\n",
            "overall the point of the movie is just to have fun and im sure that you will\n",
            "======= Samples From Class 2 =========\n",
            "while by no means a classic the directors involved do have an idea what suspense is\n",
            "good film thing is the film is named to kill for such a shame\n",
            "however the ending was really cool i say its must see i liked him lots i loved her rest of the cast do fine overall a must see\n",
            "i dont understand it really this is a good movie the supporting cast is good as well and the music in the movie is great its probably what makes the movie as enjoyable as it is\n",
            "this is an average short but its still worth watching\n",
            "not a great film but certainly an enjoyable one its worth the effort\n",
            "but they just dont understand the movie and the reason is because they understand the movie\n",
            "he was both funny and sad the script worked well too\n",
            "but the plot i heard was so great was so predictable plus the only cool character gets killed off in the middle of the movie wrong in short not much happens\n",
            "the cast is first rate the film is a great deal of fun worth a look\n",
            "======= Samples From Class 3 =========\n",
            "things come to a head and one has to keep watching to follow up on such a sequence\n",
            "he has no friends and all he wants to do is write and have someone like his writing\n",
            "often feels like you are part of an audience at a stage show\n",
            "its a well acted film and it has a good beginning that gets you involved straight away its a good film\n",
            "well this is new the name mr youre a god mr\n",
            "he will have an oscar one day it was realistic too it is all realistic\n",
            "we have seen that he has done lots of other work and think that he is doing a great job\n",
            "its not even close we need something different s s s version does have a better soundtrack than the original version s version\n",
            "it shows that mr the lead performances are very good indeed\n",
            "that should tell you something i think that despite the lack of dialogue the actors did a fantastic job on the episode\n",
            "======= Samples From Class 4 =========\n",
            "if you can do that it really is good s s s s i doubt it too bad\n",
            "but it is still dont care is it worth try it\n",
            "want action want more go out and buy this video\n",
            "i enjoyed this movie as a kid when it came out and to this day still do i am surprised not too many people know about it cool flick\n",
            "i am not a very good writer so ill keep this short\n",
            "i must say that i had wanted to see this film for a long time and i was not disappointed youll love it\n",
            "much to my surprise it wasnt about that at all\n",
            "if anyone every finds this movie i would absolutely love to see it whats not to love\n",
            "i read the book before i saw the movie i knew the movie was going to be good because the book was great i seriously recommend you see this amazing fantastic movie i know you will like it\n",
            "i kept expecting it to fall apart but it never really did\n"
          ]
        }
      ],
      "source": [
        "pi, theta = em_bmm(X_pos, K=5, num_iter=10)\n",
        "Z = e_step(X_pos, pi, theta)\n",
        "classes = np.argmax(Z, axis=1)\n",
        "for k in range(Z.shape[1]):\n",
        "    print(\"======= Samples From Class\", k, \"=========\")\n",
        "    for i in np.where(classes == k)[0][:10]:\n",
        "        print(data_pos[i][0])\n",
        "\n",
        "# TODO: Write your interpretation of the classes here\n",
        "# In summary, the BMM clustering has separated the reviews not by sentiment (all are positive), but by how that sentiment is expressed — whether it’s thoughtful, technical, humorous, nostalgic, or casual.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjMdUra0qKLG"
      },
      "source": [
        "**Task**: Fit a BMM model using the EM algorithm on only the negative\n",
        "reviews (`X_neg`).\n",
        "Use `K=5` and `num_iter=10` to cluster the data into 5 classes.\n",
        "Then, use `e_step` to compute the class responsibilities `Z`, for each data point.\n",
        "After you have computed `Z`, use the below code to print out some example reviews\n",
        "in each of the five classes.\n",
        "\n",
        "Provide an interpretation for what these classes mean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvCLuivsqKLG",
        "outputId": "3332ae8b-356a-489d-aec5-671f220e4179"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======= Samples From Class 0 =========\n",
            "this is quite simply one of the worst movies that i have ever seen in my life i know that even a low budget movie can be great but not this one what happens save your money and your time\n",
            "why is this one of the worst films ever come on mr\n",
            "this has got to be the worst horror movie i have ever seen avoid this movie\n",
            "in fact this is the his first film i have seen i am sure this movie is okay to watch as long as it is not taken too seriously\n",
            "i cant remember the worst film i have watched total waste of actors and audience time total waste of time\n",
            "no doubt she does but not in this film just a waste of movie time\n",
            "this is the worst movie ive ever seen dont waste even a minute in your life to watch this crap they use only camera in hand dont make more movies\n",
            "the zombie on the front of the dvd looks cool and scary however i cant possibly imagine anyone thinking this is anything but one of the worst movies ever made the real horror in this film is how bad it is\n",
            "its not only one of the worst movies ive seen but it is definitely the worst musical ive ever seen\n",
            "in fact this film could be used in a film class to show how not to make a suspense film dont waste your time\n",
            "======= Samples From Class 1 =========\n",
            "you want a movie that leaves the audience on the side of the bad guys\n",
            "this is a great idea for a film but it unfortunately doesnt turn out to be a great movie\n",
            "of course he has no idea what he is doing\n",
            "i wanted this movie to work so badly but it just didnt\n",
            "i wanted to love this movie how could i not love it\n",
            "she needs a man to help her the comedy too is not needed in a strong subject film like this even more so the comedy is simply not funny\n",
            "you know how it is well that is not true i just cant recommend this one\n",
            "am i right in thinking i went to see the same film as everyone else this film was terrible someone back me up\n",
            "but you can see it from here i definitely dont understand why anyone would recommend this movie no point to having made it really\n",
            "i dont understand what all of these beautiful women saw in him does this man have any shame what so ever what a mess of a movie\n",
            "======= Samples From Class 2 =========\n",
            "imagine what this film could have been like with a decent budget\n",
            "a sorry i guess that part of your life isnt over yet get well soon you half a man\n",
            "still these two women most especially know whats funny and they know how to write a funny movie\n",
            "i know ill write one just like it where everything works out okay ill work on that\n",
            "well when all is said and done we dont really care\n",
            "i wonder whether id have liked this movie when i was the characters age my guess is i wouldnt\n",
            "i could go on good no well made certainly not\n",
            "or if they have it was not very well as it should be\n",
            "thats just one example of how far outside of reality the play goes and in the make believe world of the theater it works\n",
            "what were people thinking when they made this film etc\n",
            "======= Samples From Class 3 =========\n",
            "not good and not bad either hes a very good actor and should probably be an action star or something why do they make movies like this in the first place\n",
            "after all movies for free i told myself well hes done the good and the bad as far as films and tv go when they finally take off the effects are truly horrible but the horror doesnt stop there\n",
            "this has got to be one of the worst movies ive ever seen do they laugh when they create all this ridiculous stuff or do they actually think theyre doing something interesting i wonder its the best thing to do god\n",
            "all the way though i was thinking to myself oh god why why does going into the house make her come after you it doesnt make sense you can see everything coming which just left you feeling that there was no point in watching oh shes behind her\n",
            "the reviews i read for this movie were pretty decent so i decided to check it out bad idea and i didnt care about any of them perhaps a better writer could have made the movie work there were some decent scenes in it but overall this movie was a mess\n",
            "cinematography but why stop there the plot is just poor dialog is the less said the better characters dont get me started the more you think about this the worse it gets\n",
            "i dont really know where to start not only the acting was bad the characters were incredibly stupid as well then theres the action i believe that even children know that when someone gets shot theres blood involved times theres no blood at all well i guess thats just me to make a long story short because believe me i can go on for hours about this film this is without a doubt the worst film i ever saw\n",
            "great etc the list goes on and on and on i wanted to like this movie a thats it period and it could have worked whats that they wanted their film to be different right\n",
            "we tried to get into it but the plot made very little sense even after reading the back of the dvd box over and over again the film was shot very dark and it was pretty annoying to try to figure out what was going on in each shot\n",
            "in this case it does not its a joke if you ask me some of the scenes were so long that they could have easily been cut in half hes the lead male so he should have some hero elements to his character but he does not i could go on but i dont want to waste my time the cover of the dvd is nice and thats where it ends just terrible\n",
            "======= Samples From Class 4 =========\n",
            "the direction is terrible the acting is so so rest of the cast are average at best overall not worth your time or money\n",
            "there is just one word for this film some actors of quality have small parts in this film\n",
            "what were they thinking of terrible script terrible acting i dont even feel sorry for the actors i could go on and on\n",
            "rock too bad the rest of the film could not have been as good as mr\n",
            "etc save your money and your time not entertaining at all\n",
            "its very hard to think of a worse movie with such big name actors well\n",
            "really a terrible movie its to be expected though either way the movies pretty bad and dont watch it if theres anything better on\n",
            "in this case it was a big miss this movie was truly a big waste of time\n",
            "weak acting and very predictable nothing original about it but do not waste your time with this one\n",
            "the supposed writer and director mr many scenes are also exactly the same\n"
          ]
        }
      ],
      "source": [
        "pi, theta = em_bmm(X_neg, K=5, num_iter=10)\n",
        "Z = e_step(X_neg, pi, theta)\n",
        "classes = np.argmax(Z, axis=1)\n",
        "for k in range(Z.shape[1]):\n",
        "    print(\"======= Samples From Class\", k, \"=========\")\n",
        "    for i in np.where(classes == k)[0][:10]:\n",
        "        print(data_neg[i][0])\n",
        "\n",
        "# TODO: Write your interpretation of the classes here"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}